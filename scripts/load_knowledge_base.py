"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π RAG.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç .txt, .md, .pdf —Ñ–∞–π–ª—ã.
"""

import asyncio
import os
import re
import sys
from pathlib import Path
from typing import List, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent))

from shared.rag import VectorStore, EmbeddingService
from shared.rag.vector_store import get_vector_store
from shared.utils.logger import get_logger

logger = get_logger(__name__)


class DocumentLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""

    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (–≤ —Å–∏–º–≤–æ–ª–∞—Ö)
    CHUNK_SIZE = 1000
    # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
    CHUNK_OVERLAP = 200

    def __init__(self, vector_store: VectorStore = None):
        self._vector_store = vector_store

    async def get_vector_store(self) -> VectorStore:
        if self._vector_store is None:
            self._vector_store = await get_vector_store()
        return self._vector_store

    def chunk_text(self, text: str, chunk_size: int = None, overlap: int = None) -> List[str]:
        """
        –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º.

        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è

        Returns:
            –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
        """
        chunk_size = chunk_size or self.CHUNK_SIZE
        overlap = overlap or self.CHUNK_OVERLAP

        # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
        text = text.strip()
        if not text:
            return []

        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        paragraphs = re.split(r'\n\n+', text)

        chunks = []
        current_chunk = ""

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if len(current_chunk) + len(para) + 2 <= chunk_size:
                if current_chunk:
                    current_chunk += "\n\n" + para
                else:
                    current_chunk = para
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append(current_chunk)

                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                if len(para) > chunk_size:
                    sentences = re.split(r'(?<=[.!?])\s+', para)
                    current_chunk = ""
                    for sent in sentences:
                        if len(current_chunk) + len(sent) + 1 <= chunk_size:
                            if current_chunk:
                                current_chunk += " " + sent
                            else:
                                current_chunk = sent
                        else:
                            if current_chunk:
                                chunks.append(current_chunk)
                            current_chunk = sent
                else:
                    current_chunk = para

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def read_text_file(self, file_path: Path) -> str:
        """–ü—Ä–æ—á–∏—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª."""
        encodings = ['utf-8', 'cp1251', 'latin-1']
        for encoding in encodings:
            try:
                return file_path.read_text(encoding=encoding)
            except UnicodeDecodeError:
                continue
        raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {file_path}")

    def read_markdown(self, file_path: Path) -> str:
        """–ü—Ä–æ—á–∏—Ç–∞—Ç—å Markdown —Ñ–∞–π–ª (—É–±–∏—Ä–∞–µ–º —Ä–∞–∑–º–µ—Ç–∫—É)."""
        text = self.read_text_file(file_path)
        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ markdown
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
        # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ [text](url) -> text
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)
        # –£–±–∏—Ä–∞–µ–º –∂–∏—Ä–Ω—ã–π/–∫—É—Ä—Å–∏–≤
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        text = re.sub(r'__([^_]+)__', r'\1', text)
        text = re.sub(r'_([^_]+)_', r'\1', text)
        # –£–±–∏—Ä–∞–µ–º –∫–æ–¥ –±–ª–æ–∫–∏
        text = re.sub(r'```[^`]*```', '', text)
        text = re.sub(r'`([^`]+)`', r'\1', text)
        return text

    async def load_file(
        self,
        file_path: Path,
        category: str = None,
        metadata: dict = None
    ) -> int:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –æ–¥–∏–Ω —Ñ–∞–π–ª –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π.

        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        file_path = Path(file_path)
        if not file_path.exists():
            logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        suffix = file_path.suffix.lower()

        if suffix in ['.txt', '.text']:
            text = self.read_text_file(file_path)
        elif suffix in ['.md', '.markdown']:
            text = self.read_markdown(file_path)
        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: {suffix}")
            return 0

        if not text.strip():
            logger.warning(f"–ü—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
            return 0

        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
        chunks = self.chunk_text(text)

        if not chunks:
            logger.warning(f"–ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ñ–∞–π–ª–∞: {file_path}")
            return 0

        # –ì–æ—Ç–æ–≤–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        documents = []
        for i, chunk in enumerate(chunks):
            doc = {
                "content": chunk,
                "source": file_path.name,
                "category": category,
                "chunk_index": i,
                "metadata": {
                    **(metadata or {}),
                    "file_path": str(file_path),
                    "total_chunks": len(chunks)
                }
            }
            documents.append(doc)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ –±–∞–∑—É
        store = await self.get_vector_store()
        await store.add_documents(documents)

        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {file_path.name}")
        return len(chunks)

    async def load_directory(
        self,
        dir_path: Path,
        category: str = None,
        recursive: bool = True
    ) -> dict:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.

        Args:
            dir_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            recursive: –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
        """
        dir_path = Path(dir_path)
        if not dir_path.is_dir():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {dir_path}")
            return {"error": "Directory not found"}

        stats = {
            "files_processed": 0,
            "chunks_added": 0,
            "errors": [],
            "files": []
        }

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
        patterns = ["*.txt", "*.md", "*.markdown"]
        files = []

        for pattern in patterns:
            if recursive:
                files.extend(dir_path.rglob(pattern))
            else:
                files.extend(dir_path.glob(pattern))

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤ –≤ {dir_path}")

        for file_path in files:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –∏–º–µ–Ω–∏ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                file_category = category
                if file_path.parent != dir_path:
                    file_category = file_path.parent.name

                chunks = await self.load_file(file_path, category=file_category)
                stats["files_processed"] += 1
                stats["chunks_added"] += chunks
                stats["files"].append({
                    "file": file_path.name,
                    "chunks": chunks,
                    "category": file_category
                })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}")
                stats["errors"].append(str(file_path))

        return stats


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."""
    print("=" * 60)
    print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π RAG")
    print("=" * 60)

    # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    project_root = Path(__file__).parent.parent
    knowledge_base_path = project_root / "content" / "knowledge_base"

    if not knowledge_base_path.exists():
        print(f"\n‚ùå –ü–∞–ø–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {knowledge_base_path}")
        print("\nüí° –°–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É –∏ –¥–æ–±–∞–≤—å—Ç–µ —Ç—É–¥–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã:")
        print(f"   {knowledge_base_path}")
        print("\n–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .txt, .md")
        return

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã
    files = list(knowledge_base_path.rglob("*.txt")) + list(knowledge_base_path.rglob("*.md"))

    if not files:
        print(f"\n‚ö†Ô∏è  –ü–∞–ø–∫–∞ {knowledge_base_path} –ø—É—Å—Ç–∞!")
        print("\nüí° –î–æ–±–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –ø–∞–ø–∫—É:")
        print("   - –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ NL International")
        print("   - –ú–∞—Ä–∫–µ—Ç–∏–Ω–≥-–ø–ª–∞–Ω")
        print("   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤")
        print("   - FAQ –∏ —á–∞—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã")
        print("\n–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: .txt, .md")
        return

    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ {len(files)} —Ñ–∞–π–ª–æ–≤")
    print("\n–ó–∞–≥—Ä—É–∑–∫–∞...")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
    loader = DocumentLoader()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
    stats = await loader.load_directory(knowledge_base_path)

    print("\n" + "=" * 60)
    print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–≥—Ä—É–∑–∫–∏:")
    print("=" * 60)
    print(f"  –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats['files_processed']}")
    print(f"  –ß–∞–Ω–∫–æ–≤ –¥–æ–±–∞–≤–ª–µ–Ω–æ:  {stats['chunks_added']}")

    if stats['errors']:
        print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∏ ({len(stats['errors'])}):")
        for err in stats['errors']:
            print(f"   - {err}")

    if stats['files']:
        print("\nüìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        for f in stats['files']:
            print(f"   - {f['file']}: {f['chunks']} —á–∞–Ω–∫–æ–≤ [{f['category'] or '–±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏'}]")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã
    store = await get_vector_store()
    db_stats = await store.get_stats()

    print("\n" + "=" * 60)
    print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
    print("=" * 60)
    print(f"  –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {db_stats['total_documents']}")
    print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {db_stats['embedding_dimension']}")

    if db_stats['by_category']:
        print("\n  –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for cat, count in db_stats['by_category'].items():
            print(f"    - {cat}: {count}")

    print("\n‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    asyncio.run(main())
